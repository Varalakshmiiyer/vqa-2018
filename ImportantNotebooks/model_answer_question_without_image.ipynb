{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "import torch.nn as nn\n",
    "import word2vec\n",
    "import scipy.signal\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "# If we want proper CUDA debug info.\n",
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams['axes.grid'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ninety-nine'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number to int conversion\n",
    "import inflect\n",
    "p = inflect.engine()\n",
    "p.number_to_words('99')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dir = \"/Users/jawa/Desktop/mark_ra/falconet/models/resources/\"\n",
    "glove_dim = 200\n",
    "glove_file = \"glove.twitter.27B.{}d.txt\".format(glove_dim)\n",
    "glove2word2vec(glove_input_file=glove_dir+glove_file, word2vec_output_file=\"resources/gensim_glove_vectors.txt\")\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"resources/gensim_glove_vectors.txt\", binary=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'four' in glove_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pre process model_dir\n",
    "model_dir = \"data/processed/nans,2000_maxlength,26_minwcount,0_nlp,mcb_pad,left_trainsplit,train\"\n",
    "# aid_to_ans.pickle\n",
    "aid_to_ans = pickle.load(open(os.path.join(model_dir,\"aid_to_ans.pickle\"),\"rb\"))\n",
    "# ans_to_aid.pickle\n",
    "ans_to_aid = pickle.load(open(os.path.join(model_dir,\"ans_to_aid.pickle\"),\"rb\"))\n",
    "# testdevset.pickle\n",
    "testdevset = pickle.load(open(os.path.join(model_dir,\"testdevset.pickle\"),\"rb\"))\n",
    "# testset.pickle\n",
    "testset = pickle.load(open(os.path.join(model_dir,\"testset.pickle\"),\"rb\"))\n",
    "# trainset.pickle\n",
    "trainset = pickle.load(open(os.path.join(model_dir,\"trainset.pickle\"),\"rb\"))\n",
    "# valset.pickle\n",
    "valset = pickle.load(open(os.path.join(model_dir,\"valset.pickle\"),\"rb\"))\n",
    "# wid_to_word.pickle\n",
    "wid_to_word = pickle.load(open(os.path.join(model_dir,\"wid_to_word.pickle\"),\"rb\"))\n",
    "# word_to_wid.pickle\n",
    "word_to_wid = pickle.load(open(os.path.join(model_dir,\"word_to_wid.pickle\"),\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "447793"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "len_answers = len(aid_to_ans)\n",
    "print(len_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'net',\n",
       " 'answer_aid': 934,\n",
       " 'answers': ['net'],\n",
       " 'answers_aid': [934],\n",
       " 'answers_count': [8],\n",
       " 'answers_occurence': [['net', 8], ['netting', 1], ['mesh', 1]],\n",
       " 'image_name': 'COCO_train2014_000000458752.jpg',\n",
       " 'question': 'What is this photo taken looking through?',\n",
       " 'question_id': 458752000,\n",
       " 'question_length': 7,\n",
       " 'question_wids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7],\n",
       " 'question_words': ['what',\n",
       "  'is',\n",
       "  'this',\n",
       "  'photo',\n",
       "  'taken',\n",
       "  'looking',\n",
       "  'through'],\n",
       " 'question_words_UNK': ['what',\n",
       "  'is',\n",
       "  'this',\n",
       "  'photo',\n",
       "  'taken',\n",
       "  'looking',\n",
       "  'through'],\n",
       " 'seq_length': 7}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualQuestionsDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, ds,ans_to_aid,aid_to_ans,wid_to_word,word_to_wid,image_root_dir=None,transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.question_answer_ds =  ds\n",
    "        self.ans_to_aid = ans_to_aid\n",
    "        self.aid_to_ans = aid_to_ans\n",
    "        self.wid_to_word = wid_to_word\n",
    "        self.word_to_wid = word_to_wid\n",
    "        self.image_root_dir = image_root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.question_answer_ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # self.image_root_dir\n",
    "        image_feat = None\n",
    "        question = self.question_answer_ds[idx]['question_wids']\n",
    "        \n",
    "        # question_vec = [glove_model[wid_to_word[x]] for x in question if x!=0 ]\n",
    "        question_vec=[]\n",
    "        for x in question:\n",
    "            if x == 0:\n",
    "                question_vec.append(glove_dim*[0])\n",
    "            else:\n",
    "                word = wid_to_word[x]\n",
    "                if word.isdigit():\n",
    "                    word = p.number_to_words(word)\n",
    "                if word in glove_model:\n",
    "                    question_vec.append(glove_model[word])\n",
    "                else:\n",
    "                    question_vec.append(glove_dim*[0])\n",
    "                \n",
    "        answer_id = self.question_answer_ds[idx]['answer_aid']\n",
    "        y = np.zeros(len_answers)\n",
    "        y[answer_id]=1\n",
    "        question_vec = np.asarray(question_vec)\n",
    "        \n",
    "        return question_vec,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VisualQuestionsDataset(trainset,ans_to_aid,aid_to_ans,wid_to_word,word_to_wid)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,  batch_size=5, shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQA_BASIC_WITHOUT_IMAGE_MODEL(nn.Module):\n",
    "    def __init__(self, input_size,hidden_size =2048,n_layers=2):\n",
    "        super().__init__()\n",
    "        self.rnn =  nn.LSTM(input_size = input_size, hidden_size = hidden_size , num_layers =n_layers)\n",
    "        self.linear = nn.Linear(hidden_size,len_answers)\n",
    "        self.sofmax = nn.Softmax()\n",
    "    def forward(self,x,input=None):\n",
    "        x,  final_state = self.rnn(x)\n",
    "        # picking the last elemnt from the sequence as output to the fc\n",
    "        x = x[-1,:,:]\n",
    "        x = self.linear(x)\n",
    "        x = self.sofmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQA_BASIC_WITHOUT_IMAGE_MODEL(glove_dim,1024,2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_f = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_step(sample_batch):\n",
    "    model.train()\n",
    "    inputs, target = sample_batch\n",
    "    inputs = inputs.permute(1,0,2)\n",
    "    inputs = Variable(inputs)\n",
    "    #print('input_size',inputs.size())\n",
    "    target = Variable(target)\n",
    "    print('target_size',target.size())\n",
    "    inputs = inputs.float()\n",
    "    output = model(inputs)\n",
    "    print('output size', output.size())\n",
    "    print('target size', target.size())\n",
    "    loss = loss_f(output,target.long())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_step(sample_batch):\n",
    "    model.eval()\n",
    "    inputs, target = sample_batch\n",
    "    inputs = inputs.permute(1,0,2)\n",
    "    inputs = Variable(inputs)\n",
    "    target = Variable(target)\n",
    "    inputs = inputs.float()\n",
    "    output = model(inputs)\n",
    "    loss = loss_f(output,target)\n",
    "    acc = accuracy(output,target)\n",
    "    return (loss.data[0],acc.data[0]/target.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(test,target):\n",
    "    print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 5000\n",
    "num_steps_per_summary = 250\n",
    "\n",
    "for i_batch, sample_batch in enumerate(train_loader):    \n",
    "    x,y = sample_batch\n",
    "    train_step(sample_batch)\n",
    "    if step % num_steps_per_summary == 0:\n",
    "        train_loss = eval_step(sample_batch)\n",
    "        val_loss = eval_step(sample_batch)\n",
    "        steps.append(step)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f'Step {step:05d} / {num_steps:05d}. Train loss: {train_loss:.3f}. Val loss: {val_loss:.3f}.')\n",
    "        print('Samples:', processed_samples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7649"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_wid['sparsely']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
