{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import argparse\n",
    "from collections import Counter\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir_vqa = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_train_qa    = os.path.join(dir_vqa, 'interim', 'train_questions_annotations.json')\n",
    "path_val_qa      = os.path.join(dir_vqa, 'interim', 'val_questions_annotations.json')\n",
    "path_trainval_qa = os.path.join(dir_vqa, 'interim', 'trainval_questions_annotations.json')\n",
    "path_test_q      = os.path.join(dir_vqa, 'interim', 'test_questions.json')\n",
    "path_testdev_q   = os.path.join(dir_vqa, 'interim', 'testdev_questions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jawa/Desktop/dl_project/vqa-2018/data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotations and questions...\n"
     ]
    }
   ],
   "source": [
    "print('Loading annotations and questions...')\n",
    "annotations_train_1 = json.load(open(os.path.join(dir_vqa,  'annotations', 'v2_mscoco_train2014_annotations.json'), 'r'))\n",
    "annotations_train_2 = json.load(open(os.path.join(dir_vqa,  'annotations', 'abstract_v002_train2015_annotations.json'), 'r'))\n",
    "annotations_train_3 = json.load(open(os.path.join(dir_vqa,  'annotations', 'abstract_v002_train2017_annotations.json'), 'r'))\n",
    "\n",
    "questions_train_1 = json.load(open(os.path.join(dir_vqa,  'Questions', 'v2_OpenEnded_mscoco_train2014_questions.json'), 'r'))\n",
    "questions_train_2 = json.load(open(os.path.join(dir_vqa,  'Questions', 'OpenEnded_abstract_v002_train2015_questions.json'), 'r'))\n",
    "questions_train_3 = json.load(open(os.path.join(dir_vqa,  'Questions', 'OpenEnded_abstract_v002_train2017_questions.json'), 'r'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annotations_val_1 = json.load(open(os.path.join(dir_vqa,  'annotations', 'v2_mscoco_val2014_annotations.json'), 'r'))\n",
    "annotations_val_2 = json.load(open(os.path.join(dir_vqa,  'annotations', 'abstract_v002_val2015_annotations.json'), 'r'))\n",
    "annotations_val_3 = json.load(open(os.path.join(dir_vqa,  'annotations', 'abstract_v002_val2017_annotations.json'), 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions_val_1 = json.load(open(os.path.join(dir_vqa,  'Questions', 'v2_OpenEnded_mscoco_val2014_questions.json'), 'r'))\n",
    "questions_val_2 = json.load(open(os.path.join(dir_vqa,  'Questions', 'OpenEnded_abstract_v002_val2015_questions.json'), 'r'))\n",
    "questions_val_3 = json.load(open(os.path.join(dir_vqa,  'Questions', 'OpenEnded_abstract_v002_val2017_questions.json'), 'r'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['info', 'license', 'data_subtype', 'annotations', 'data_type'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_val_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val2014\n",
      "mscoco\n",
      "{'description': 'This is v2.0 of the VQA dataset.', 'url': 'http://visualqa.org', 'version': '2.0', 'year': 2017, 'contributor': 'VQA Team', 'date_created': '2017-04-26 17:00:44'}\n"
     ]
    }
   ],
   "source": [
    "print(annotations_val_1['data_subtype'])\n",
    "print(annotations_val_1['data_type'])\n",
    "print(annotations_val_1['info'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['info', 'task_type', 'data_type', 'license', 'data_subtype', 'questions'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_val_1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'This is v2.0 of the VQA dataset.', 'url': 'http://visualqa.org', 'version': '2.0', 'year': 2017, 'contributor': 'VQA Team', 'date_created': '2017-04-26 17:00:44'}\n",
      "Open-Ended\n",
      "mscoco\n",
      "{'url': 'http://creativecommons.org/licenses/by/4.0/', 'name': 'Creative Commons Attribution 4.0 International License'}\n",
      "val2014\n"
     ]
    }
   ],
   "source": [
    "print(questions_val_1['info'])\n",
    "print(questions_val_1['task_type'])\n",
    "print(questions_val_1['data_type'])\n",
    "print(questions_val_1['license'])\n",
    "print(questions_val_1['data_subtype'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_subtype(split='train'):\n",
    "    if split in ['train', 'val']:\n",
    "        return split + '2014'\n",
    "    else:\n",
    "        return 'test2015'\n",
    "    \n",
    "def get_image_name_old(subtype='train2014', image_id='1', format='%s/COCO_%s_%012d.jpg'):\n",
    "    return format%(subtype, subtype, image_id)\n",
    "\n",
    "def get_image_name(subtype='train2014', image_id='1', format='COCO_%s_%012d.jpg'):\n",
    "    return format%(subtype, image_id)\n",
    "\n",
    "def interim(questions, split='train', annotations=[]):\n",
    "    print('Interim', split)\n",
    "    data = []\n",
    "    for i in range(len(questions)):\n",
    "        row = {}\n",
    "        row['question_id'] = questions[i]['question_id']\n",
    "        row['image_name'] = get_image_name(get_subtype(split), questions[i]['image_id'])\n",
    "        row['question'] = questions[i]['question']\n",
    "        if split in ['train', 'val', 'trainval']:\n",
    "            row['answer'] = annotations[i]['multiple_choice_answer']\n",
    "            answers = []\n",
    "            for ans in annotations[i]['answers']:\n",
    "                answers.append(ans['answer'])\n",
    "            row['answers_occurence'] = Counter(answers).most_common()\n",
    "        data.append(row)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interim val\n",
      "Interim val\n",
      "Interim val\n"
     ]
    }
   ],
   "source": [
    "val_merge_1 = interim(questions= questions_val_1['questions'],annotations = annotations_train_1['annotations'], split = 'val')\n",
    "val_merge_2 = interim(questions= questions_val_2['questions'],annotations = annotations_train_2['annotations'], split = 'val')\n",
    "val_merge_3 = interim(questions= questions_val_3['questions'],annotations = annotations_train_3['annotations'], split = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_id': 262148000, 'image_name': 'COCO_val2014_000000262148.jpg', 'question': 'Where is he looking?', 'answer': 'net', 'answers_occurence': [('net', 8), ('netting', 1), ('mesh', 1)]}\n",
      "{'question_id': 275780, 'image_name': 'COCO_val2014_000000027578.jpg', 'question': 'Is the dog asleep?', 'answer': 'man', 'answers_occurence': [('man', 7), ('old person', 1), ('old man', 1), ('grandpa', 1)]}\n",
      "{'question_id': 289402, 'image_name': 'COCO_val2014_000000028940.jpg', 'question': 'Is it daylight?', 'answer': 'yes', 'answers_occurence': [('yes', 9), ('no', 1)]}\n"
     ]
    }
   ],
   "source": [
    "print(val_merge_1[0])\n",
    "print(val_merge_2[0])\n",
    "print(val_merge_3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 214354 b = 30000 c = 11328 sum = 255682\n"
     ]
    }
   ],
   "source": [
    "a = len(val_merge_1)\n",
    "b = len(val_merge_2)\n",
    "c =len(val_merge_3)\n",
    "print(f\"a = {a:05d} b = {b:05d} c = {c:05d} sum = {a+b+c:05d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valset = val_merge_1+val_merge_2+val_merge_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(dir_vqa, 'interim')):\n",
    "    os.makedirs(os.path.join(dir_vqa, 'interim'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 255682\n",
      "Write ./interim/val_questions_annotations.json\n"
     ]
    }
   ],
   "source": [
    "print('Train size %d'%len(valset))\n",
    "print('Write', path_val_qa) \n",
    "# json.dump(trainset, open(path_train_qa, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import pdb\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "#import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_answers(examples, nans=3000):\n",
    "    counts = {}\n",
    "    for ex in examples:\n",
    "        ans = ex['answer'] \n",
    "        counts[ans] = counts.get(ans, 0) + 1\n",
    "\n",
    "    cw = sorted([(count,w) for w,count in counts.items()], reverse=True)\n",
    "    print('Top answer and their counts:'    )\n",
    "    print('\\n'.join(map(str,cw[:20])))\n",
    "\n",
    "    vocab = []\n",
    "    for i in range(nans):\n",
    "        vocab.append(cw[i][1])\n",
    "    return vocab[:nans]\n",
    "\n",
    "def remove_examples(examples, ans_to_aid):\n",
    "    new_examples = []\n",
    "    for i, ex in enumerate(examples):\n",
    "        if ex['answer'] in ans_to_aid:\n",
    "            new_examples.append(ex)\n",
    "    print('Number of examples reduced from %d to %d '%(len(examples), len(new_examples)))\n",
    "    return new_examples\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return [i for i in re.split(r\"([-.\\\"',:? !\\$#@~()*&\\^%;\\[\\]/\\\\\\+<>\\n=])\", sentence) if i!='' and i!=' ' and i!='\\n'];\n",
    "\n",
    "def tokenize_mcb(s):\n",
    "    t_str = s.lower()\n",
    "    for i in [r'\\?',r'\\!',r'\\'',r'\\\"',r'\\$',r'\\:',r'\\@',r'\\(',r'\\)',r'\\,',r'\\.',r'\\;']:\n",
    "        t_str = re.sub( i, '', t_str)\n",
    "    for i in [r'\\-',r'\\/']:\n",
    "        t_str = re.sub( i, ' ', t_str)\n",
    "    q_list = re.sub(r'\\?','',t_str.lower()).split(' ')\n",
    "    q_list = list(filter(lambda x: len(x) > 0, q_list))\n",
    "    return q_list\n",
    "\n",
    "def preprocess_questions(examples, nlp='nltk'):\n",
    "    if nlp == 'nltk':\n",
    "        from nltk.tokenize import word_tokenize\n",
    "    print('Example of generated tokens after preprocessing some questions:')\n",
    "    for i, ex in enumerate(examples):\n",
    "        s = ex['question']\n",
    "        if nlp == 'nltk':\n",
    "            ex['question_words'] = word_tokenize(str(s).lower())\n",
    "        elif nlp == 'mcb':\n",
    "            ex['question_words'] = tokenize_mcb(s)\n",
    "        else:\n",
    "            ex['question_words'] = tokenize(s)\n",
    "        if i < 10:\n",
    "            print(ex['question_words'])\n",
    "        if i % 1000 == 0:\n",
    "            sys.stdout.write(\"processing %d/%d (%.2f%% done)   \\r\" %  (i, len(examples), i*100.0/len(examples)) )\n",
    "            sys.stdout.flush() \n",
    "    return examples\n",
    "\n",
    "def remove_long_tail_train(examples, minwcount=0):\n",
    "    # Replace words which are in the long tail (counted less than 'minwcount' times) by the UNK token.\n",
    "    # Also create vocab, a list of the final words.\n",
    "\n",
    "    # count up the number of words\n",
    "    counts = {}\n",
    "    for ex in examples:\n",
    "        for w in ex['question_words']:\n",
    "            counts[w] = counts.get(w, 0) + 1\n",
    "    cw = sorted([(count,w) for w, count in counts.items()], reverse=True)\n",
    "    print('Top words and their counts:')\n",
    "    print('\\n'.join(map(str,cw[:20])))\n",
    "\n",
    "    total_words = sum(counts.values())\n",
    "    print('Total words:', total_words)\n",
    "    bad_words = [w for w,n in counts.items() if n <= minwcount]\n",
    "    vocab     = [w for w,n in counts.items() if n > minwcount]\n",
    "    bad_count = sum(counts[w] for w in bad_words)\n",
    "    print('Number of bad words: %d/%d = %.2f%%' % (len(bad_words), len(counts), len(bad_words)*100.0/len(counts)))\n",
    "    print('Number of words in vocab would be %d' % (len(vocab), ))\n",
    "    print('Number of UNKs: %d/%d = %.2f%%' % (bad_count, total_words, bad_count*100.0/total_words))\n",
    "\n",
    "    print('Insert the special UNK token')\n",
    "    vocab.append('UNK')\n",
    "    for ex in examples:\n",
    "        words = ex['question_words']\n",
    "        question = [w if counts.get(w,0) > minwcount else 'UNK' for w in words]\n",
    "        ex['question_words_UNK'] = question\n",
    "\n",
    "    return examples, vocab\n",
    "\n",
    "def remove_long_tail_test(examples, word_to_wid):\n",
    "    for ex in examples:\n",
    "        ex['question_words_UNK'] = [w if w in word_to_wid else 'UNK' for w in ex['question_words']]\n",
    "    return examples\n",
    "\n",
    "def encode_question(examples, word_to_wid, maxlength=26, pad='left'):\n",
    "    # Add to tuple question_wids and question_length\n",
    "    for i, ex in enumerate(examples):\n",
    "        ex['question_length'] = min(maxlength, len(ex['question_words_UNK'])) # record the length of this sequence\n",
    "        ex['question_wids'] = [0]*maxlength\n",
    "        for k, w in enumerate(ex['question_words_UNK']):\n",
    "            if k < maxlength:\n",
    "                if pad == 'right':\n",
    "                    ex['question_wids'][k] = word_to_wid[w]\n",
    "                else:   #['pad'] == 'left'\n",
    "                    new_k = k + maxlength - len(ex['question_words_UNK'])\n",
    "                    ex['question_wids'][new_k] = word_to_wid[w]\n",
    "                ex['seq_length'] = len(ex['question_words_UNK'])\n",
    "    return examples\n",
    "\n",
    "def encode_answer(examples, ans_to_aid):\n",
    "    print('Warning: aid of answer not in vocab is 1999')\n",
    "    for i, ex in enumerate(examples):\n",
    "        ex['answer_aid'] = ans_to_aid.get(ex['answer'], 1999) # -1 means answer not in vocab\n",
    "    return examples\n",
    "\n",
    "def encode_answers_occurence(examples, ans_to_aid):\n",
    "    for i, ex in enumerate(examples):\n",
    "        answers = []\n",
    "        answers_aid = []\n",
    "        answers_count = []\n",
    "        for ans in ex['answers_occurence']:\n",
    "            aid = ans_to_aid.get(ans[0], -1) # -1 means answer not in vocab\n",
    "            if aid != -1:\n",
    "                answers.append(ans[0])\n",
    "                answers_aid.append(aid) \n",
    "                answers_count.append(ans[1])\n",
    "        ex['answers']       = answers\n",
    "        ex['answers_aid']   = answers_aid\n",
    "        ex['answers_count'] = answers_count\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vqa_process(nans=2000, nlp='mcb',pad='left', minwcount =0,trainsplit='train',params_dir = dir_vqa, maxlength=30):\n",
    "    \n",
    "    path_train = os.path.join(dir_vqa, 'interim', trainsplit +'_questions_annotations.json')\n",
    "    if params['trainsplit'] == 'train':\n",
    "        path_val = os.path.join(params['dir'], 'interim', 'val_questions_annotations.json')\n",
    "#     path_test    = os.path.join(params['dir'], 'interim', 'test_questions.json')\n",
    "#     path_testdev = os.path.join(params['dir'], 'interim', 'testdev_questions.json')\n",
    " \n",
    "    # An example is a tuple (question, image, answer)\n",
    "    # /!\\ test and test-dev have no answer\n",
    "    trainset = json.load(open(path_train, 'r'))\n",
    "    # read it from file in real code \n",
    "    top_answers = get_top_answers(trainset, nans)\n",
    "    aid_to_ans = [a for i,a in enumerate(top_answers)]\n",
    "    ans_to_aid = {a:i for i,a in enumerate(top_answers)}\n",
    "    # Remove examples if answer is not in top answers\n",
    "    trainset = remove_examples(trainset, ans_to_aid)\n",
    "\n",
    "    # Add 'question_words' to the initial tuple\n",
    "    trainset = preprocess_questions(trainset, nlp)\n",
    "     if params['trainsplit'] == 'train':\n",
    "        valset = preprocess_questions(valset, nlp)\n",
    "        testset    = preprocess_questions(testset, nlp)\n",
    "#     testdevset = preprocess_questions(testdevset, nlp)\n",
    "\n",
    "    # Also process top_words which contains a UNK char\n",
    "    trainset, top_words = remove_long_tail_train(trainset, minwcount)\n",
    "    wid_to_word = {i+1:w for i,w in enumerate(top_words)}\n",
    "    word_to_wid = {w:i+1 for i,w in enumerate(top_words)}\n",
    "\n",
    "#     if params['trainsplit'] == 'train':\n",
    "#         valset = remove_long_tail_test(valset, word_to_wid)\n",
    "#     testset    = remove_long_tail_test(testset, word_to_wid)\n",
    "#     testdevset = remove_long_tail_test(testdevset, word_to_wid)\n",
    "\n",
    "    trainset = encode_question(trainset, word_to_wid, maxlength, pad)\n",
    "#     if params['trainsplit'] == 'train':\n",
    "#         valset = encode_question(valset, word_to_wid, maxlength,pad)\n",
    "#     testset    = encode_question(testset, word_to_wid, maxlength,pad)\n",
    "#     testdevset = encode_question(testdevset, word_to_wid, maxlength,pad)\n",
    "\n",
    "    trainset = encode_answer(trainset, ans_to_aid)\n",
    "    trainset = encode_answers_occurence(trainset, ans_to_aid)\n",
    "#     if params['trainsplit'] == 'train':\n",
    "#         valset = encode_answer(valset, ans_to_aid)\n",
    "#         valset = encode_answers_occurence(valset, ans_to_aid)\n",
    "    return trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top answer and their counts:\n",
      "(110107, 'yes')\n",
      "(103899, 'no')\n",
      "(15711, '2')\n",
      "(14474, '1')\n",
      "(9705, 'white')\n",
      "(7909, '3')\n",
      "(6707, 'red')\n",
      "(6058, 'blue')\n",
      "(5444, '0')\n",
      "(5348, 'black')\n",
      "(4717, '4')\n",
      "(4331, 'brown')\n",
      "(4062, 'green')\n",
      "(3395, 'yellow')\n",
      "(2741, '5')\n",
      "(2386, 'gray')\n",
      "(2028, 'nothing')\n",
      "(1997, 'right')\n",
      "(1767, 'left')\n",
      "(1726, 'baseball')\n",
      "Number of examples reduced from 525812 to 483694 \n",
      "Example of generated tokens after preprocessing some questions:\n",
      "['what', 'is', 'this', 'photo', 'taken', 'looking', 'through']\n",
      "['what', 'position', 'is', 'this', 'man', 'playing']\n",
      "['what', 'color', 'is', 'the', 'players', 'shirt']\n",
      "['is', 'this', 'man', 'a', 'professional', 'baseball', 'player']\n",
      "['what', 'color', 'is', 'the', 'snow']\n",
      "['what', 'is', 'the', 'person', 'doing']\n",
      "['what', 'color', 'is', 'the', 'persons', 'headwear']\n",
      "['what', 'is', 'in', 'the', 'persons', 'hand']\n",
      "['is', 'the', 'dog', 'waiting']\n",
      "['is', 'the', 'dog', 'looking', 'at', 'a', 'tennis', 'ball', 'or', 'frisbee']\n",
      "Top words and their counts:9.86% done)   \n",
      "(366674, 'the')\n",
      "(302846, 'is')\n",
      "(174724, 'what')\n",
      "(114175, 'are')\n",
      "(84520, 'this')\n",
      "(73427, 'in')\n",
      "(67624, 'on')\n",
      "(65125, 'a')\n",
      "(60960, 'how')\n",
      "(57667, 'many')\n",
      "(55175, 'of')\n",
      "(50409, 'color')\n",
      "(45104, 'there')\n",
      "(24945, 'man')\n",
      "(22406, 'does')\n",
      "(20931, 'to')\n",
      "(19835, 'people')\n",
      "(18455, 'picture')\n",
      "(14618, 'wearing')\n",
      "(13852, 'it')\n",
      "Total words: 2976324\n",
      "Number of bad words: 0/13390 = 0.00%\n",
      "Number of words in vocab would be 13390\n",
      "Number of UNKs: 0/2976324 = 0.00%\n",
      "Insert the special UNK token\n",
      "Warning: aid of answer not in vocab is 1999\n"
     ]
    }
   ],
   "source": [
    "final_set = vqa_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mark_ra",
   "language": "python",
   "name": "mark_ra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
